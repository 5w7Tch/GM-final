{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGpdwSQ_T1YX"
      },
      "source": [
        "# VAE Training on CIFAR-10\n",
        "**Variational Autoencoder**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzsmQh4OT1YZ"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axS4Tu0aT1YZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/5w7Tch/GM-final\n",
        "%cd GM-final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lwe3QLBT1Ya"
      },
      "outputs": [],
      "source": [
        "!pip install wandb tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tuWFocWT1Ya"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from src.models import VAE\n",
        "from src.losses import vae_loss\n",
        "from src.data import get_dataloader\n",
        "from src.utils import show_samples, save_samples\n",
        "\n",
        "import wandb\n",
        "wandb.login(key=\"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\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAKmi7W5T1Ya"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrhjLjTjT1Ya"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'latent_dim': 128,\n",
        "    'beta': 1.0,  # KL weight (beta-VAE)\n",
        "\n",
        "    'epochs': 200,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-4,\n",
        "\n",
        "    'sample_every': 10,\n",
        "    'save_every': 25,\n",
        "\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "wandb.init(\n",
        "    project=\"ML2-NCSN-CIFAR10\",\n",
        "    config=config,\n",
        "    name=f\"VAE_latent{config['latent_dim']}_beta{config['beta']}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RjtzXSQT1Yb"
      },
      "source": [
        "## 3. Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA-2nRv7T1Yb"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(config['seed'])\n",
        "torch.cuda.manual_seed_all(config['seed'])\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('samples', exist_ok=True)\n",
        "\n",
        "train_loader = get_dataloader(batch_size=config['batch_size'])\n",
        "\n",
        "# TODO: Initialize model after implementing VAE\n",
        "model = VAE(latent_dim=config['latent_dim']).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config['lr'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QaOFIT_T1Yb"
      },
      "source": [
        "## 4. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5t4Msd_T1Yc"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    pbar = tqdm(train_loader)\n",
        "    for images, _ in pbar:\n",
        "        images = images.to(device)\n",
        "\n",
        "        x_recon, mu, log_var = model(images)\n",
        "        loss, recon_loss, kl_loss = vae_loss(\n",
        "            images, x_recon, mu, log_var, config['beta']\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        wandb.log({\n",
        "            \"loss/total\": loss.item(),\n",
        "            \"loss/recon\": recon_loss.item(),\n",
        "            \"loss/kl\": kl_loss.item(),\n",
        "            \"epoch\": epoch + 1\n",
        "        })\n",
        "\n",
        "\n",
        "        pbar.set_description(\n",
        "            f\"Epoch {epoch+1} | Loss {loss.item():.4f}\"\n",
        "        )\n",
        "\n",
        "    if (epoch + 1) % config['sample_every'] == 0:\n",
        "        samples = model.sample(64, device)\n",
        "        wandb.log({\n",
        "            \"samples\": wandb.Image(\n",
        "                (samples + 1) / 2, caption=f\"Epoch {epoch+1}\"\n",
        "            )\n",
        "        })\n",
        "        show_samples(samples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJDnrigkT1Yc"
      },
      "source": [
        "## 5. Evaluation\n",
        "\n",
        "FID calculation, reconstruction visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models import inception_v3\n",
        "from torchvision.transforms import Resize\n",
        "from scipy.linalg import sqrtm\n",
        "\n",
        "# -------------------------\n",
        "# Reconstruction visualization\n",
        "# -------------------------\n",
        "def show_reconstructions(model, dataloader, device, n=8):\n",
        "    model.eval()\n",
        "    images, _ = next(iter(dataloader))\n",
        "    images = images[:n].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x_recon, _, _ = model(images)\n",
        "\n",
        "    images = (images + 1) / 2\n",
        "    x_recon = (x_recon + 1) / 2\n",
        "\n",
        "    fig, axes = plt.subplots(2, n, figsize=(2*n, 4))\n",
        "\n",
        "    for i in range(n):\n",
        "        axes[0, i].imshow(images[i].permute(1, 2, 0).cpu())\n",
        "        axes[0, i].axis(\"off\")\n",
        "\n",
        "        axes[1, i].imshow(x_recon[i].permute(1, 2, 0).cpu())\n",
        "        axes[1, i].axis(\"off\")\n",
        "\n",
        "    axes[0, 0].set_ylabel(\"Original\")\n",
        "    axes[1, 0].set_ylabel(\"Reconstruction\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Sample generation\n",
        "# -------------------------\n",
        "def generate_samples(model, n_samples, device, batch_size=64):\n",
        "    model.eval()\n",
        "    samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples // batch_size):\n",
        "            z = torch.randn(batch_size, model.latent_dim).to(device)\n",
        "            x = model.decoder(z)\n",
        "            samples.append(x)\n",
        "\n",
        "    samples = torch.cat(samples, dim=0)\n",
        "    samples = (samples + 1) / 2  # [-1,1] â†’ [0,1]\n",
        "    return samples\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Inception feature extraction\n",
        "# -------------------------\n",
        "def get_inception_features(images, device):\n",
        "    model = inception_v3(pretrained=True, transform_input=False)\n",
        "    model.fc = torch.nn.Identity()\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    resize = Resize((299, 299))\n",
        "    features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(images), 32):\n",
        "            batch = images[i:i+32].to(device)\n",
        "            batch = resize(batch)\n",
        "            feat = model(batch)\n",
        "            features.append(feat.cpu())\n",
        "\n",
        "    return torch.cat(features, dim=0).numpy()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# FID computation\n",
        "# -------------------------\n",
        "def calculate_fid(real_feats, fake_feats):\n",
        "    mu_r, sigma_r = real_feats.mean(0), np.cov(real_feats, rowvar=False)\n",
        "    mu_f, sigma_f = fake_feats.mean(0), np.cov(fake_feats, rowvar=False)\n",
        "\n",
        "    diff = mu_r - mu_f\n",
        "    covmean = sqrtm(sigma_r @ sigma_f)\n",
        "\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    fid = diff @ diff + np.trace(sigma_r + sigma_f - 2 * covmean)\n",
        "    return fid\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Run evaluation\n",
        "# -------------------------\n",
        "show_reconstructions(model, train_loader, device)\n",
        "\n",
        "real_images, _ = next(iter(train_loader))\n",
        "real_images = (real_images + 1) / 2\n",
        "\n",
        "fake_images = generate_samples(model, 1024, device)\n",
        "\n",
        "real_feats = get_inception_features(real_images, device)\n",
        "fake_feats = get_inception_features(fake_images, device)\n",
        "\n",
        "fid = calculate_fid(real_feats, fake_feats)\n",
        "print(\"FID:\", fid)\n",
        "wandb.log({\"FID\": fid})\n",
        "wandb.log({\n",
        "    \"reconstructions\": wandb.Image(fig)\n",
        "})\n",
        "wandb.finish()\n",
        "\n"
      ],
      "metadata": {
        "id": "ltrnIKOt2pxo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}