{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGpdwSQ_T1YX"
      },
      "source": [
        "# VAE Training on CIFAR-10\n",
        "**Variational Autoencoder**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzsmQh4OT1YZ"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axS4Tu0aT1YZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/YOUR_USERNAME/ML2_final.git\n",
        "%cd ML2_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lwe3QLBT1Ya"
      },
      "outputs": [],
      "source": [
        "!pip install wandb tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tuWFocWT1Ya"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from src.models import VAE\n",
        "from src.losses import vae_loss\n",
        "from src.data import get_dataloader, denormalize\n",
        "from src.utils import show_samples, save_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAKmi7W5T1Ya"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrhjLjTjT1Ya"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'latent_dim': 128,\n",
        "    'beta': 1.0,  # KL weight (beta-VAE)\n",
        "\n",
        "    'epochs': 200,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-4,\n",
        "\n",
        "    'sample_every': 10,\n",
        "    'save_every': 25,\n",
        "\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RjtzXSQT1Yb"
      },
      "source": [
        "## 3. Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA-2nRv7T1Yb"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(config['seed'])\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('samples', exist_ok=True)\n",
        "\n",
        "train_loader = get_dataloader(batch_size=config['batch_size'])\n",
        "\n",
        "# TODO: Initialize model after implementing VAE\n",
        "model = VAE(latent_dim=config['latent_dim']).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config['lr'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QaOFIT_T1Yb"
      },
      "source": [
        "## 4. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5t4Msd_T1Yc"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    pbar = tqdm(train_loader)\n",
        "    for images, _ in pbar:\n",
        "        images = images.to(device)\n",
        "\n",
        "        x_recon, mu, log_var = model(images)\n",
        "        loss, recon_loss, kl_loss = vae_loss(\n",
        "            images, x_recon, mu, log_var, config['beta']\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_description(\n",
        "            f\"Epoch {epoch+1} | Loss {loss.item():.4f}\"\n",
        "        )\n",
        "\n",
        "    if (epoch + 1) % config['sample_every'] == 0:\n",
        "        samples = model.sample(64, device)\n",
        "        show_samples(samples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJDnrigkT1Yc"
      },
      "source": [
        "## 5. Evaluation\n",
        "\n",
        "FID calculation, reconstruction visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(model, n_samples, device, batch_size=64):\n",
        "    model.eval()\n",
        "    samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples // batch_size):\n",
        "            z = torch.randn(batch_size, model.latent_dim).to(device)\n",
        "            x = model.decoder(z)\n",
        "            samples.append(x)\n",
        "\n",
        "    samples = torch.cat(samples, dim=0)\n",
        "    samples = (samples + 1) / 2  # [-1,1] â†’ [0,1]\n",
        "    return samples\n",
        "\n",
        "from torchvision.models import inception_v3\n",
        "from torchvision.transforms import Resize\n",
        "\n",
        "def get_inception_features(images, device):\n",
        "    model = inception_v3(pretrained=True, transform_input=False)\n",
        "    model.fc = torch.nn.Identity()\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    resize = Resize((299, 299))\n",
        "    features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(images), 32):\n",
        "            batch = images[i:i+32].to(device)\n",
        "            batch = resize(batch)\n",
        "            feat = model(batch)\n",
        "            features.append(feat.cpu())\n",
        "\n",
        "    return torch.cat(features, dim=0)\n",
        "\n",
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "\n",
        "def calculate_fid(real_feats, fake_feats):\n",
        "    mu_r, sigma_r = real_feats.mean(0), np.cov(real_feats, rowvar=False)\n",
        "    mu_f, sigma_f = fake_feats.mean(0), np.cov(fake_feats, rowvar=False)\n",
        "\n",
        "    diff = mu_r - mu_f\n",
        "    covmean = sqrtm(sigma_r @ sigma_f)\n",
        "\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    fid = diff @ diff + np.trace(sigma_r + sigma_f - 2 * covmean)\n",
        "    return fid\n",
        "\n",
        "real_images, _ = next(iter(train_loader))\n",
        "real_images = (real_images + 1) / 2\n",
        "\n",
        "fake_images = generate_samples(model, 1024, device)\n",
        "\n",
        "real_feats = get_inception_features(real_images, device).numpy()\n",
        "fake_feats = get_inception_features(fake_images, device).numpy()\n",
        "\n",
        "fid = calculate_fid(real_feats, fake_feats)\n",
        "print(\"FID:\", fid)\n"
      ],
      "metadata": {
        "id": "ltrnIKOt2pxo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}