{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/5w7Tch/GM-final/blob/main/notebooks/train_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGpdwSQ_T1YX"
      },
      "source": [
        "# VAE Training on CIFAR-10\n",
        "**Variational Autoencoder**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzsmQh4OT1YZ"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "axS4Tu0aT1YZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ca0861-bf65-45b9-f7ee-afaa220ada2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GM-final'...\n",
            "remote: Enumerating objects: 111, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 111 (delta 51), reused 43 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (111/111), 5.68 MiB | 4.22 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "/content/GM-final\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/5w7Tch/GM-final\n",
        "%cd GM-final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_Lwe3QLBT1Ya"
      },
      "outputs": [],
      "source": [
        "!pip install wandb tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-tuWFocWT1Ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2720ebb-e7bf-40de-fa12-4504cedb0da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meghib22\u001b[0m (\u001b[33mnurch22-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from src.models import VAE\n",
        "from src.losses import vae_loss\n",
        "from src.data import get_dataloader\n",
        "from src.utils import show_samples, save_samples\n",
        "\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAKmi7W5T1Ya"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jrhjLjTjT1Ya",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "0db7c348-7934-4796-86d3-48f0b8c4b9b3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/GM-final/wandb/run-20260131_204043-ikr8yvga</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nurch22-free-university-of-tbilisi-/ML2-NCSN-CIFAR10/runs/ikr8yvga' target=\"_blank\">VAE_latent64_beta0.1</a></strong> to <a href='https://wandb.ai/nurch22-free-university-of-tbilisi-/ML2-NCSN-CIFAR10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nurch22-free-university-of-tbilisi-/ML2-NCSN-CIFAR10' target=\"_blank\">https://wandb.ai/nurch22-free-university-of-tbilisi-/ML2-NCSN-CIFAR10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nurch22-free-university-of-tbilisi-/ML2-NCSN-CIFAR10/runs/ikr8yvga' target=\"_blank\">https://wandb.ai/nurch22-free-university-of-tbilisi-/ML2-NCSN-CIFAR10/runs/ikr8yvga</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nurch22-free-university-of-tbilisi-/ML2-NCSN-CIFAR10/runs/ikr8yvga?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7944af1b7e60>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "config = {\n",
        "    'latent_dim': 64,\n",
        "    'beta': 0.1,  # KL weight (beta-VAE)\n",
        "\n",
        "    'epochs': 60,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-4,\n",
        "\n",
        "    'sample_every': 10,\n",
        "    'save_every': 25,\n",
        "\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "wandb.init(\n",
        "    project=\"ML2-NCSN-CIFAR10\",\n",
        "    config=config,\n",
        "    name=f\"VAE_latent{config['latent_dim']}_beta{config['beta']}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RjtzXSQT1Yb"
      },
      "source": [
        "## 3. Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kA-2nRv7T1Yb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01db4e3-ef91-4ebc-91e4-bdcb2d8e1159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(config['seed'])\n",
        "torch.cuda.manual_seed_all(config['seed'])\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('samples', exist_ok=True)\n",
        "\n",
        "train_loader = get_dataloader(batch_size=config['batch_size'])\n",
        "\n",
        "# TODO: Initialize model after implementing VAE\n",
        "model = VAE(latent_dim=config['latent_dim']).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config['lr'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QaOFIT_T1Yb"
      },
      "source": [
        "## 4. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5t4Msd_T1Yc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    pbar = tqdm(train_loader)\n",
        "    for images, _ in pbar:\n",
        "        images = images.to(device)\n",
        "\n",
        "        x_recon, mu, log_var = model(images)\n",
        "        loss, recon_loss, kl_loss = vae_loss(\n",
        "            images, x_recon, mu, log_var, config['beta']\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        wandb.log({\n",
        "            \"loss/total\": loss.item(),\n",
        "            \"loss/recon\": recon_loss.item(),\n",
        "            \"loss/kl\": kl_loss.item(),\n",
        "            \"epoch\": epoch + 1\n",
        "        })\n",
        "\n",
        "\n",
        "        pbar.set_description(\n",
        "            f\"Epoch {epoch+1} | Loss {loss.item():.4f}\"\n",
        "        )\n",
        "\n",
        "    if (epoch + 1) % config['sample_every'] == 0:\n",
        "        samples = model.sample(64, device)\n",
        "\n",
        "        # Normalize samples to [0, 1] for display and logging\n",
        "        normalized_samples = (samples + 1) / 2\n",
        "\n",
        "        # Create a grid of samples for logging to wandb as a single image\n",
        "        # make_grid expects [N, C, H, W] and returns [C, H_grid, W_grid]\n",
        "        grid = make_grid(normalized_samples.cpu(), nrow=8, padding=2, normalize=False)\n",
        "\n",
        "        # Convert to HWC format numpy array for wandb.Image\n",
        "        # .permute(1, 2, 0) converts [C, H, W] to [H, W, C]\n",
        "        grid_np = grid.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"samples\": wandb.Image(grid_np, caption=f\"Epoch {epoch+1}\")\n",
        "            }\n",
        "        )\n",
        "        show_samples(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJDnrigkT1Yc"
      },
      "source": [
        "## 5. Evaluation\n",
        "\n",
        "FID calculation, reconstruction visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models import inception_v3\n",
        "from torchvision.transforms import Resize\n",
        "from scipy.linalg import sqrtm\n",
        "\n",
        "# -------------------------\n",
        "# Reconstruction visualization\n",
        "# -------------------------\n",
        "def show_reconstructions(model, dataloader, device, n=8):\n",
        "    model.eval()\n",
        "    images, _ = next(iter(dataloader))\n",
        "    images = images[:n].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x_recon, _, _ = model(images)\n",
        "\n",
        "    images = (images + 1) / 2\n",
        "    x_recon = (x_recon + 1) / 2\n",
        "\n",
        "    fig, axes = plt.subplots(2, n, figsize=(2*n, 4))\n",
        "\n",
        "    for i in range(n):\n",
        "        axes[0, i].imshow(images[i].permute(1, 2, 0).cpu())\n",
        "        axes[0, i].axis(\"off\")\n",
        "\n",
        "        axes[1, i].imshow(x_recon[i].permute(1, 2, 0).cpu())\n",
        "        axes[1, i].axis(\"off\")\n",
        "\n",
        "    axes[0, 0].set_ylabel(\"Original\")\n",
        "    axes[1, 0].set_ylabel(\"Reconstruction\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Sample generation\n",
        "# -------------------------\n",
        "def generate_samples(model, n_samples, device, batch_size=64):\n",
        "    model.eval()\n",
        "    samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples // batch_size):\n",
        "            z = torch.randn(batch_size, model.latent_dim).to(device)\n",
        "            x = model.decoder(z)\n",
        "            samples.append(x)\n",
        "\n",
        "    samples = torch.cat(samples, dim=0)\n",
        "    samples = (samples + 1) / 2  # [-1,1] → [0,1]\n",
        "    return samples\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Inception feature extraction\n",
        "# -------------------------\n",
        "def get_inception_features(images, device):\n",
        "    model = inception_v3(pretrained=True, transform_input=False)\n",
        "    model.fc = torch.nn.Identity()\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    resize = Resize((299, 299))\n",
        "    features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(images), 32):\n",
        "            batch = images[i:i+32].to(device)\n",
        "            batch = resize(batch)\n",
        "            feat = model(batch)\n",
        "            features.append(feat.cpu())\n",
        "\n",
        "    return torch.cat(features, dim=0).numpy()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# FID computation\n",
        "# -------------------------\n",
        "def calculate_fid(real_feats, fake_feats):\n",
        "    mu_r, sigma_r = real_feats.mean(0), np.cov(real_feats, rowvar=False)\n",
        "    mu_f, sigma_f = fake_feats.mean(0), np.cov(fake_feats, rowvar=False)\n",
        "\n",
        "    diff = mu_r - mu_f\n",
        "    covmean = sqrtm(sigma_r @ sigma_f)\n",
        "\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    fid = diff @ diff + np.trace(sigma_r + sigma_f - 2 * covmean)\n",
        "    return fid\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Run evaluation\n",
        "# -------------------------\n",
        "fig = show_reconstructions(model, train_loader, device)\n",
        "\n",
        "real_images, _ = next(iter(train_loader))\n",
        "real_images = (real_images + 1) / 2\n",
        "\n",
        "fake_images = generate_samples(model, 1024, device)\n",
        "\n",
        "real_feats = get_inception_features(real_images, device)\n",
        "fake_feats = get_inception_features(fake_images, device)\n",
        "\n",
        "fid = calculate_fid(real_feats, fake_feats)\n",
        "print(\"FID:\", fid)\n",
        "wandb.log({\"FID\": fid})\n",
        "wandb.log({\n",
        "    \"reconstructions\": wandb.Image(fig)\n",
        "})\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "ltrnIKOt2pxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sRgWDEONM9BG"
      },
      "execution_count": 8,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}