{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCSN Training on CIFAR-10\n",
    "**Noise Conditional Score Network**\n",
    "\n",
    "With automatic checkpoint resume for Colab interruptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project folder in Drive\n",
    "import os\n",
    "DRIVE_PATH = '/content/drive/MyDrive/ML2_NCSN'\n",
    "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_PATH}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_PATH}/samples', exist_ok=True)\n",
    "print(f\"Saving to: {DRIVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content')\n",
    "!rm -rf ML2_final GM-final\n",
    "!git clone https://github.com/5w7Tch/GM-final.git\n",
    "%cd GM-final\n",
    "!pwd\n",
    "!ls src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.models import NCSN, get_sigmas\n",
    "from src.losses import anneal_dsm_loss\n",
    "from src.sampling import generate_samples\n",
    "from src.data import get_dataloader, denormalize\n",
    "from src.utils import EMA, show_samples, save_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb (optional but recommended)\n",
    "USE_WANDB = True\n",
    "\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Model\n",
    "    'num_features': 128,\n",
    "    'num_classes': 10,\n",
    "    \n",
    "    # Noise schedule\n",
    "    'sigma_begin': 1.0,\n",
    "    'sigma_end': 0.01,\n",
    "    \n",
    "    # Training\n",
    "    'epochs': 200,\n",
    "    'batch_size': 128,\n",
    "    'lr': 1e-4,\n",
    "    'ema_decay': 0.999,\n",
    "    \n",
    "    # Sampling\n",
    "    'n_steps_each': 100,\n",
    "    'step_lr': 2e-5,\n",
    "    \n",
    "    # Checkpointing (IMPORTANT for Colab)\n",
    "    'save_every_n_epochs': 5,      # Save checkpoint every N epochs\n",
    "    'sample_every': 10,            # Generate samples every N epochs\n",
    "    'keep_last_n_checkpoints': 3,  # Keep only last N checkpoints to save space\n",
    "    \n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Checkpoint Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Find the latest checkpoint file.\"\"\"\n",
    "    checkpoints = glob.glob(f\"{checkpoint_dir}/epoch_*.pt\")\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    # Sort by epoch number\n",
    "    checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    return checkpoints[-1]\n",
    "\n",
    "\n",
    "def save_checkpoint(path, model, ema, optimizer, scheduler, epoch, global_step, sigmas, config):\n",
    "    \"\"\"Save complete training state.\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'ema_shadow': ema.shadow,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'sigmas': sigmas.cpu(),\n",
    "        'config': config\n",
    "    }, path)\n",
    "    print(f\"âœ“ Saved checkpoint: {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, ema, optimizer, scheduler, device):\n",
    "    \"\"\"Load training state from checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    ema.shadow = {k: v.to(device) for k, v in checkpoint['ema_shadow'].items()}\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    return checkpoint['epoch'], checkpoint['global_step']\n",
    "\n",
    "\n",
    "def cleanup_old_checkpoints(checkpoint_dir, keep_n=3):\n",
    "    \"\"\"Keep only the last N checkpoints to save Drive space.\"\"\"\n",
    "    checkpoints = glob.glob(f\"{checkpoint_dir}/epoch_*.pt\")\n",
    "    if len(checkpoints) <= keep_n:\n",
    "        return\n",
    "    \n",
    "    checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    for ckpt in checkpoints[:-keep_n]:\n",
    "        os.remove(ckpt)\n",
    "        print(f\"Removed old checkpoint: {ckpt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config['seed'])\n",
    "\n",
    "# Data\n",
    "train_loader = get_dataloader(batch_size=config['batch_size'], train=True)\n",
    "print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "# Model\n",
    "model = NCSN(\n",
    "    num_classes=config['num_classes'],\n",
    "    num_features=config['num_features']\n",
    ").to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Sigmas\n",
    "sigmas = get_sigmas(\n",
    "    config['sigma_begin'],\n",
    "    config['sigma_end'],\n",
    "    config['num_classes']\n",
    ").to(device)\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config['epochs'] * len(train_loader)\n",
    ")\n",
    "\n",
    "# EMA\n",
    "ema = EMA(model, decay=config['ema_decay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resume from Checkpoint (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoint\n",
    "CHECKPOINT_DIR = f\"{DRIVE_PATH}/checkpoints\"\n",
    "SAMPLE_DIR = f\"{DRIVE_PATH}/samples\"\n",
    "\n",
    "latest_ckpt = get_latest_checkpoint(CHECKPOINT_DIR)\n",
    "\n",
    "if latest_ckpt:\n",
    "    print(f\"\\nðŸ”„ Found checkpoint: {latest_ckpt}\")\n",
    "    start_epoch, global_step = load_checkpoint(\n",
    "        latest_ckpt, model, ema, optimizer, scheduler, device\n",
    "    )\n",
    "    start_epoch += 1  # Start from next epoch\n",
    "    print(f\"âœ“ Resuming from epoch {start_epoch}, step {global_step}\")\n",
    "else:\n",
    "    print(\"\\nðŸ†• No checkpoint found. Starting fresh.\")\n",
    "    start_epoch = 0\n",
    "    global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb (with resume support)\n",
    "if USE_WANDB:\n",
    "    # Use a fixed run ID so we can resume the same run\n",
    "    RUN_ID = 'ncsn_cifar10_main'  # Change this if you want a new run\n",
    "    \n",
    "    wandb.init(\n",
    "        project='ML2-NCSN',\n",
    "        id=RUN_ID,\n",
    "        resume='allow',  # Resume if run exists\n",
    "        config=config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Training from epoch {start_epoch} to {config['epochs']}\")\n",
    "print(f\"Checkpoints saved to: {CHECKPOINT_DIR}\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "for epoch in range(start_epoch, config['epochs']):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "    \n",
    "    for images, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        loss = anneal_dsm_loss(model, images, sigmas)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        ema.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        global_step += 1\n",
    "        \n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}', lr=f'{scheduler.get_last_lr()[0]:.2e}')\n",
    "        \n",
    "        if USE_WANDB and global_step % 50 == 0:\n",
    "            wandb.log({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]}, step=global_step)\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.log({'epoch_loss': avg_loss, 'epoch': epoch+1}, step=global_step)\n",
    "    \n",
    "    # === SAVE CHECKPOINT ===\n",
    "    if (epoch + 1) % config['save_every_n_epochs'] == 0:\n",
    "        ckpt_path = f\"{CHECKPOINT_DIR}/epoch_{epoch+1:04d}.pt\"\n",
    "        save_checkpoint(\n",
    "            ckpt_path, model, ema, optimizer, scheduler,\n",
    "            epoch, global_step, sigmas, config\n",
    "        )\n",
    "        cleanup_old_checkpoints(CHECKPOINT_DIR, config['keep_last_n_checkpoints'])\n",
    "    \n",
    "    # === GENERATE SAMPLES ===\n",
    "    if (epoch + 1) % config['sample_every'] == 0:\n",
    "        print(\"Generating samples...\")\n",
    "        ema.apply_shadow()\n",
    "        model.eval()\n",
    "        \n",
    "        samples = generate_samples(\n",
    "            model, sigmas, n_samples=64,\n",
    "            n_steps_each=config['n_steps_each'],\n",
    "            step_lr=config['step_lr'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        sample_path = f\"{SAMPLE_DIR}/epoch_{epoch+1:04d}.png\"\n",
    "        save_samples(samples, sample_path)\n",
    "        show_samples(samples, title=f'Epoch {epoch+1}')\n",
    "        \n",
    "        if USE_WANDB:\n",
    "            wandb.log({'samples': wandb.Image(sample_path)}, step=global_step)\n",
    "        \n",
    "        ema.restore()\n",
    "        model.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model with EMA weights\n",
    "ema.apply_shadow()\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'sigmas': sigmas.cpu(),\n",
    "    'config': config\n",
    "}, f\"{DRIVE_PATH}/final_model.pt\")\n",
    "\n",
    "print(f\"âœ“ Final model saved to {DRIVE_PATH}/final_model.pt\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Final Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# High quality samples with more steps\n",
    "final_samples = generate_samples(\n",
    "    model, sigmas, n_samples=64,\n",
    "    n_steps_each=200,\n",
    "    step_lr=2e-5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "save_samples(final_samples, f\"{DRIVE_PATH}/final_samples.png\")\n",
    "show_samples(final_samples, title='Final Generated Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Resume Guide\n",
    "\n",
    "If Colab disconnects:\n",
    "1. **Reconnect** to a new runtime\n",
    "2. **Run all cells from the top** - it will automatically detect and resume from the latest checkpoint in Drive\n",
    "\n",
    "Your checkpoints are safely stored in Google Drive at:\n",
    "`/content/drive/MyDrive/ML2_NCSN/checkpoints/`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
